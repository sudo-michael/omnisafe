{"Train/Entropy":-2.109264612197876,"Train/Epoch":499,"Loss/Loss_reward_critic/Delta":0.001021040603518486,"Metrics/EpRet":37.56639099121094,"Time/FPS":120.40444946289062,"_runtime":32331.454554315,"Value/reward":3.7662031650543213,"Train/LR":0,"Metrics/LagrangeMultiplier/Max":0,"_timestamp":1.7454545347729726e+09,"Time/Total":32328.369140625,"Loss/Loss_pi/Delta":0.004408226348459721,"Train/StopIter":40,"_wandb":{"runtime":32331},"Value/cost":2.3204106582852546e-06,"Loss/Loss_cost_critic":1.319118841935918e-10,"Train/PolicyRatio/Std":0.009244350716471672,"Time/Update":128.7719268798828,"Value/Adv":0.1269833892583847,"Train/KL":0.004211748484522104,"TotalEnvSteps":1e+07,"Loss/Loss_pi":-0.009804487228393555,"Loss/Loss_reward_critic":0.02970600128173828,"Train/PolicyRatio/Min":1.0004055500030518,"Metrics/EpLen":1000,"Metrics/LagrangeMultiplier":0,"Metrics/EpCost":0,"Metrics/LagrangeMultiplier/Min":0,"Train/PolicyRatio":1.0004055500030518,"Train/PolicyStd":0.029491743072867393,"Loss/Loss_cost_critic/Delta":5.396655344824808e-13,"_step":500,"Time/Epoch":166.10682678222656,"Time/Rollout":37.334835052490234,"Metrics/LagrangeMultiplier/Std":0,"Train/PolicyRatio/Max":1.0004055500030518}