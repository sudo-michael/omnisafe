{"Train/Epoch":499,"_wandb":{"runtime":32557},"Time/Epoch":206.9925994873047,"Metrics/LagrangeMultiplier":0,"_timestamp":1.7454547602990263e+09,"_runtime":32557.232351862,"Train/KL":0.0043724896386265755,"Train/StopIter":40,"Loss/Loss_cost_critic":1.3234462137301506e-10,"Time/Update":154.61843872070312,"Train/Entropy":-2.1180622577667236,"Train/PolicyRatio/Std":0.009662085212767124,"Value/reward":3.9444468021392822,"Loss/Loss_pi/Delta":0.005264776758849621,"Time/Total":32553.875,"_step":500,"Train/PolicyRatio/Max":0.9999395608901978,"Train/PolicyStd":0.02995803952217102,"Loss/Loss_reward_critic/Delta":0.0022617075592279434,"Metrics/EpRet":38.71303176879883,"Time/Rollout":52.37409973144531,"Train/PolicyRatio/Min":0.9999395608901978,"Loss/Loss_reward_critic":0.02451309747993946,"TotalEnvSteps":1e+07,"Train/LR":0,"Loss/Loss_pi":-0.009898033924400806,"Value/cost":-2.625985189297353e-07,"Metrics/EpCost":0,"Value/Adv":-0.11793579161167145,"Time/FPS":96.62181091308594,"Loss/Loss_cost_critic/Delta":5.155598170603071e-14,"Train/PolicyRatio":0.9999395608901978,"Metrics/EpLen":1000}