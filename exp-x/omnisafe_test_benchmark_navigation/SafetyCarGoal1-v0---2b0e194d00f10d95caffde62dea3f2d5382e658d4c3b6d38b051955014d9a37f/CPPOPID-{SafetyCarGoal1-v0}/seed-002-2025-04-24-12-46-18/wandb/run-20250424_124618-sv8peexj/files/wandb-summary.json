{"_timestamp":1.7455618910188737e+09,"Train/Entropy":-0.4672139883041382,"_wandb":{"runtime":37912},"Metrics/EpRet":8.499405860900879,"Loss/Loss_cost_critic":0.47514602541923523,"Metrics/LagrangeMultiplier":11.64149284362793,"Train/PolicyRatio/Std":0.007249645888805389,"Train/KL":0.003064017044380307,"Value/reward":1.0591988563537598,"_step":500,"Train/Epoch":499,"Time/Epoch":189.26449584960938,"Loss/Loss_pi/Delta":0.012830477207899094,"Loss/Loss_reward_critic":0.02297067455947399,"Train/PolicyRatio/Max":1.0005472898483276,"TotalEnvSteps":1e+07,"Time/Update":139.8508758544922,"Time/Total":37912.10546875,"Loss/Loss_cost_critic/Delta":0.0006922483444213867,"Time/Rollout":49.413570404052734,"Train/StopIter":40,"Metrics/EpLen":1000,"Train/PolicyStd":0.15231233835220337,"_runtime":37912.893323619,"Value/cost":3.811487913131714,"Value/Adv":-0.056051433086395264,"Time/FPS":105.6722183227539,"Loss/Loss_pi":-0.019275620579719543,"Train/PolicyRatio":1.0005472898483276,"Train/PolicyRatio/Min":1.0005472898483276,"Loss/Loss_reward_critic/Delta":0.0006828941404819489,"Train/LR":0,"Metrics/EpCost":31.65999984741211}