{"Train/Epoch":499,"Train/KL":0.0031184416729956865,"Train/PolicyRatio":1.0006996393203735,"Train/PolicyRatio/Min":1.0006996393203735,"Time/Epoch":186.40658569335938,"Value/reward":1.364264965057373,"Train/PolicyRatio/Max":1.0006996393203735,"Train/PolicyRatio/Std":0.007948321290314198,"Time/Rollout":47.0577278137207,"Train/StopIter":40,"Value/cost":2.5506575107574463,"Train/PolicyStd":0.14422334730625153,"Time/Total":38468.390625,"Time/FPS":107.29234313964844,"Loss/Loss_reward_critic/Delta":0.008261090144515038,"Loss/Loss_pi/Delta":0.010301904752850533,"Loss/Loss_cost_critic":0.32349327206611633,"_step":500,"_runtime":38469.152384043,"Loss/Loss_cost_critic/Delta":-0.017051219940185547,"Loss/Loss_pi":-0.010641049593687057,"Time/Update":139.3488006591797,"Value/Adv":0.04689720273017883,"Metrics/LagrangeMultiplier":1.884169101715088,"TotalEnvSteps":1e+07,"Train/Entropy":-0.5392528772354126,"Metrics/EpRet":11.52624797821045,"Metrics/EpLen":1000,"_wandb":{"runtime":38469},"Loss/Loss_reward_critic":0.038179390132427216,"_timestamp":1.7455657099953163e+09,"Metrics/EpCost":21.34000015258789,"Train/LR":0}