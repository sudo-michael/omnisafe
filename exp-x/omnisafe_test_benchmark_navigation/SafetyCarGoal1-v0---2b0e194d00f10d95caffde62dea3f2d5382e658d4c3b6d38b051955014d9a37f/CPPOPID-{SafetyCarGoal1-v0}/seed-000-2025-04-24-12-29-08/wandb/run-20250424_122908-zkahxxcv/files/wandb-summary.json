{"_timestamp":1.745561721787323e+09,"Loss/Loss_reward_critic/Delta":0.0021543223410844803,"Train/KL":0.002621744992211461,"Time/Epoch":200.38211059570312,"Train/PolicyRatio/Max":1.0008279085159302,"_runtime":38772.917727733,"Value/reward":1.3808361291885376,"Train/StopIter":40,"Loss/Loss_pi":-0.014460695907473564,"Metrics/EpLen":1000,"Metrics/EpRet":12.778741836547852,"Time/Rollout":52.866939544677734,"Train/PolicyStd":0.1466551572084427,"Value/Adv":0.3014484643936157,"Train/Epoch":499,"Train/PolicyRatio/Min":1.0008279085159302,"_wandb":{"runtime":38772},"Time/FPS":99.80931091308594,"Time/Total":38772.16796875,"Loss/Loss_cost_critic":0.43030667304992676,"Loss/Loss_pi/Delta":0.0015841014683246613,"Train/Entropy":-0.5019519329071045,"_step":500,"Metrics/EpCost":26.06999969482422,"TotalEnvSteps":1e+07,"Loss/Loss_reward_critic":0.02966787852346897,"Time/Update":147.51510620117188,"Loss/Loss_cost_critic/Delta":0.13192853331565857,"Train/LR":0,"Metrics/LagrangeMultiplier":2.904254198074341,"Value/cost":2.2352254390716553,"Train/PolicyRatio":1.0008280277252197,"Train/PolicyRatio/Std":0.00705148559063673}