{"Train/Entropy":0.31602737307548523,"TotalEnvSteps":1e+06,"Train/StopIter":10,"Train/PolicyRatio":0.9997045397758484,"Metrics/EpCost":0.3199999928474426,"Time/FPS":232.8315887451172,"Loss/Loss_cost_critic":0.6730229258537292,"Loss/Loss_reward_critic/Delta":-0.7814311981201172,"_runtime":3954.052763283,"Train/PolicyRatio/Max":0.9997045397758484,"Train/KL":0.0038386061787605286,"_timestamp":1.7449470026846366e+09,"_step":50,"Misc/Penalty":0.0004051864380016923,"Value/reward":72.62989807128906,"Train/PolicyStd":0.3321002125740051,"Train/LR":0,"Train/Epoch":49,"Loss/Loss_pi/Delta":0.0016008168458938599,"Time/Epoch":85.89900207519531,"Time/Update":37.69281005859375,"Loss/Loss_cost_critic/Delta":0.2836613655090332,"Loss/Loss_pi":-0.003991925623267889,"Value/cost":-4.517175197601318,"Train/PolicyRatio/Min":0.9997045397758484,"Value/Adv":0.09190168976783752,"Time/Rollout":48.20613098144531,"_wandb":{"runtime":3954},"Metrics/EpRet":620.6931762695312,"Metrics/EpLen":912.9500122070312,"Loss/Loss_reward_critic":16.95969009399414,"Time/Total":3953.328125,"Train/PolicyRatio/Std":0.007038287818431854}