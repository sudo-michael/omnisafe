{"Train/PolicyRatio/Min":1.0001527070999146,"_runtime":3669.038261268,"Loss/Loss_reward_critic/Delta":0.3058643341064453,"Train/StopIter":10,"Value/Adv":-0.1539757251739502,"Train/LR":0,"Time/Rollout":42.85196304321289,"Time/Total":3668.276611328125,"Train/Epoch":49,"_timestamp":1.7449466150973728e+09,"Time/FPS":260.9770202636719,"Loss/Loss_cost_critic":0.843250036239624,"Loss/Loss_pi":-0.005214542616158724,"Value/cost":-5.0901689529418945,"Misc/Penalty":0.0004147656145505607,"Train/PolicyRatio/Std":0.008812857791781425,"Train/KL":0.004612336400896311,"Loss/Loss_reward_critic":18.291833877563477,"Metrics/EpLen":932.530029296875,"Time/Update":33.78307342529297,"Loss/Loss_pi/Delta":0.0023971167393028736,"_step":50,"Train/Entropy":0.3371826112270355,"Value/reward":76.25425720214844,"Loss/Loss_cost_critic/Delta":0.44942188262939453,"Train/PolicyRatio/Max":1.0001527070999146,"_wandb":{"runtime":3669},"Metrics/EpCost":0.8899999856948853,"Train/PolicyStd":0.339313805103302,"Metrics/EpRet":663.514404296875,"TotalEnvSteps":1e+06,"Time/Epoch":76.63510131835938,"Train/PolicyRatio":1.0001527070999146}