{"Train/Entropy":0.43533891439437866,"Loss/Loss_reward_critic":13.669018745422363,"Value/reward":94.81375122070312,"Train/PolicyStd":0.38180410861968994,"Train/PolicyRatio/Std":0.00911133736371994,"Time/Update":42.544830322265625,"Time/FPS":261.563232421875,"Train/KL":0.003772784722968936,"Train/PolicyRatio":0.999122679233551,"Train/PolicyRatio/Max":0.9991226196289062,"TotalEnvSteps":1e+06,"Loss/Loss_pi_cost":0,"Value/cost":0.5002251267433167,"Loss/Loss_pi":-0.004493558779358864,"Metrics/EpLen":1000,"Time/Epoch":76.46334838867188,"_step":50,"Value/Adv":0.248418390750885,"Train/StopIter":10,"_runtime":3835.70451878,"Loss/Loss_cost_critic/Delta":-0.004650980234146118,"Time/Rollout":33.91846466064453,"Train/LR":0,"Metrics/EpRet":1019.15771484375,"Loss/Loss_cost_critic":0.08210407942533493,"Time/Total":3835.027587890625,"_timestamp":1.7449596137137132e+09,"_wandb":{"runtime":3835},"Loss/Loss_pi/Delta":0.0020154030062258244,"Train/Epoch":49,"Loss/Loss_pi_cost/Delta":0,"Train/PolicyRatio/Min":0.9991226196289062,"Loss/Loss_reward_critic/Delta":0.4134054183959961,"Metrics/EpCost":6.679999828338623}