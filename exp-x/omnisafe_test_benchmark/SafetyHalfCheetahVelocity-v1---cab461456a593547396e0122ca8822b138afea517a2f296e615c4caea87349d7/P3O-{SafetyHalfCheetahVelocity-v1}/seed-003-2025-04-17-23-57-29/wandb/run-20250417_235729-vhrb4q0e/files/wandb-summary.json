{"Loss/Loss_pi_cost":0,"Train/PolicyRatio/Min":1.000443458557129,"Train/PolicyRatio/Std":0.012390654534101486,"Train/PolicyRatio":1.000443458557129,"_runtime":3675.631150986,"_step":50,"Loss/Loss_pi/Delta":0.0018637883476912975,"Metrics/EpCost":1.4700000286102295,"Metrics/EpLen":1000,"Loss/Loss_pi":-0.00739418575540185,"Train/Entropy":0.6564489006996155,"Time/FPS":267.25848388671875,"Loss/Loss_cost_critic":0.04584771767258644,"Metrics/EpRet":800.2731323242188,"_wandb":{"runtime":3675},"Loss/Loss_reward_critic/Delta":-3.9146347045898438,"Time/Epoch":74.83392333984375,"Loss/Loss_cost_critic/Delta":0.017825933173298836,"TotalEnvSteps":1e+06,"Time/Rollout":32.92156982421875,"Train/PolicyRatio/Max":1.000443458557129,"Train/PolicyStd":0.47070783376693726,"Value/cost":0.4037935137748718,"_timestamp":1.744963125362968e+09,"Train/LR":0,"Train/KL":0.005738468840718269,"Time/Update":41.912296295166016,"Loss/Loss_reward_critic":37.7869758605957,"Value/Adv":-0.027070842683315277,"Train/StopIter":10,"Loss/Loss_pi_cost/Delta":0,"Value/reward":75.67046356201172,"Train/Epoch":49,"Time/Total":3674.837158203125}