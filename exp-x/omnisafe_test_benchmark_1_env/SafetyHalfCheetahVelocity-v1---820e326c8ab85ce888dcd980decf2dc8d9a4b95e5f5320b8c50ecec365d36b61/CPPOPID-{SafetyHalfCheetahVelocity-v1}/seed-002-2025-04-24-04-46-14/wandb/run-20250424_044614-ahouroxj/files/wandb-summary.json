{"Train/PolicyRatio/Max":0.9995028972625732,"Train/KL":0.006839974783360958,"Loss/Loss_cost_critic":2.1037378311157227,"Value/Adv":-0.006220906972885132,"Train/PolicyRatio/Min":0.9995028972625732,"Metrics/LagrangeMultiplier":0.7431817054748535,"Time/Total":11193.138671875,"_wandb":{"runtime":11193},"Value/reward":125.85395050048828,"Metrics/EpRet":1375.716552734375,"_step":50,"Loss/Loss_reward_critic":57.83329772949219,"Time/FPS":77.66548919677734,"Metrics/EpCost":57.040000915527344,"Time/Epoch":257.5146484375,"Train/StopIter":40,"Value/cost":4.229568958282471,"Train/PolicyRatio":0.9995028972625732,"Metrics/EpLen":1000,"Loss/Loss_cost_critic/Delta":0.1346299648284912,"Loss/Loss_pi/Delta":0.005772108677774668,"Train/PolicyRatio/Std":0.013070473447442055,"Loss/Loss_pi":-0.006103624124079943,"_timestamp":1.7455063682790215e+09,"Loss/Loss_reward_critic/Delta":0.6944999694824219,"Train/PolicyStd":0.4484536349773407,"Train/Entropy":0.6102942228317261,"Time/Rollout":122.85102081298828,"Train/LR":0,"_runtime":11193.876751497,"Train/Epoch":49,"TotalEnvSteps":1e+06,"Time/Update":134.6635284423828}