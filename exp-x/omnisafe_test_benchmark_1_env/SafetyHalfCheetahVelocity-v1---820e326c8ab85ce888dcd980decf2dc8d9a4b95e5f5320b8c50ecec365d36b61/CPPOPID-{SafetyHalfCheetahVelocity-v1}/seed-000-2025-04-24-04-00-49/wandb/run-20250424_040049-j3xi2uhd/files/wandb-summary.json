{"Loss/Loss_reward_critic":16.538490295410156,"Train/StopIter":40,"Train/PolicyStd":0.38697436451911926,"Time/Rollout":122.93785858154297,"Value/cost":0.558086633682251,"_runtime":11006.180345149,"Train/PolicyRatio/Std":0.012909687124192715,"Loss/Loss_reward_critic/Delta":3.29122257232666,"Metrics/EpCost":7.840000152587891,"Train/Entropy":0.45183807611465454,"Metrics/EpLen":1000,"Metrics/LagrangeMultiplier":0,"Train/PolicyRatio/Min":1.0006377696990967,"Train/KL":0.007344364654272795,"TotalEnvSteps":1e+06,"Train/PolicyRatio/Max":1.0006377696990967,"Metrics/EpRet":1107.18212890625,"Loss/Loss_pi/Delta":0.003792896866798401,"Value/Adv":-0.24543902277946472,"Loss/Loss_cost_critic":0.14098626375198364,"Train/PolicyRatio":1.0006377696990967,"Value/reward":101.15383911132812,"Loss/Loss_pi":-0.014798106625676155,"Train/LR":0,"Time/Epoch":263.2404479980469,"Train/Epoch":49,"Time/Update":140.3025360107422,"Time/FPS":75.97616577148438,"Time/Total":11005.306640625,"_wandb":{"runtime":11006},"Loss/Loss_cost_critic/Delta":0.017989560961723328,"_step":50,"_timestamp":1.7455034554803417e+09}