{"Loss/Loss_pi_cost":0,"Train/StopIter":10,"Loss/Loss_cost_critic":0.5477684736251831,"_timestamp":1.7454595873867228e+09,"Time/Update":41.038448333740234,"Train/LR":0,"Time/Total":8372.21875,"Loss/Loss_pi/Delta":0.0019745035097002983,"Train/PolicyRatio":0.9998968839645386,"Value/Adv":0.3196161091327667,"Loss/Loss_cost_critic/Delta":-0.27826207876205444,"Metrics/EpRet":42.432830810546875,"Loss/Loss_pi_cost/Delta":0,"Loss/Loss_reward_critic/Delta":-1.4623908996582031,"Metrics/EpCost":1.5099999904632568,"Train/Entropy":0.5620681643486023,"Train/PolicyStd":0.42457449436187744,"_step":50,"Time/Epoch":139.1719207763672,"Train/PolicyRatio/Max":0.9998968839645386,"Train/Epoch":49,"Time/FPS":143.7071533203125,"TotalEnvSteps":1e+06,"Time/Rollout":98.13340759277344,"Loss/Loss_pi":-0.00482488377019763,"_wandb":{"runtime":8373},"Value/reward":11.227052688598633,"Train/PolicyRatio/Min":0.9998968839645386,"Metrics/EpLen":114.62999725341797,"Train/PolicyRatio/Std":0.007885446771979332,"_runtime":8373.159084559,"Train/KL":0.004306791350245476,"Loss/Loss_reward_critic":55.02607727050781,"Value/cost":0.7045359015464783}