{"_runtime":10690.371904036,"Time/Total":10689.4296875,"Metrics/LagrangeMultiplier/Min":0,"_wandb":{"runtime":10690},"_step":50,"Time/Update":157.86761474609375,"TotalEnvSteps":1e+06,"Time/Epoch":295.202392578125,"Loss/Loss_reward_critic/Delta":5.56048583984375,"Train/PolicyRatio":0.9986386895179749,"Loss/Loss_pi/Delta":0.006331155076622963,"Train/PolicyRatio/Max":0.9986386299133301,"Metrics/LagrangeMultiplier/Std":0,"Metrics/EpRet":27.12567710876465,"Metrics/EpCost":2.009999990463257,"_timestamp":1.7454512131006985e+09,"Value/cost":0.8120231628417969,"Train/PolicyRatio/Min":0.9986386299133301,"Value/reward":2.755187511444092,"Metrics/EpLen":71.0999984741211,"Train/KL":0.007955826818943024,"Train/PolicyStd":0.4265861213207245,"Train/Epoch":49,"Loss/Loss_cost_critic":0.9977719783782959,"Loss/Loss_cost_critic/Delta":0.1982162594795227,"Train/StopIter":40,"Time/Rollout":137.33473205566406,"Value/Adv":-0.20505082607269287,"Loss/Loss_pi":-0.016432849690318108,"Time/FPS":67.7501220703125,"Train/LR":0,"Loss/Loss_reward_critic":40.7607421875,"Train/PolicyRatio/Std":0.013828335329890251,"Metrics/LagrangeMultiplier/Max":0,"Train/Entropy":0.5668066740036011,"Metrics/LagrangeMultiplier":0}