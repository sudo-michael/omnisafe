{"Value/reward":95.41704559326172,"Train/PolicyRatio":0.9987964630126953,"Loss/Loss_pi_cost":91.83304595947266,"Train/Entropy":1.02101731300354,"Train/StopIter":1,"Metrics/EpRet":488.30438232421875,"Loss/Loss_reward_critic":3.2694122791290283,"Value/Adv":0.2247646450996399,"Time/Epoch":126.2892074584961,"_timestamp":1.7454889694155784e+09,"Time/Update":4.231448173522949,"Loss/Loss_reward_critic/Delta":0.5063900947570801,"Train/LR":0,"Train/KL":0.05084903538227081,"Loss/Loss_pi":0.038802776485681534,"Loss/Loss_cost_critic/Delta":0.861015796661377,"Metrics/EpCost":29.6299991607666,"Train/Epoch":49,"_runtime":7259.874651478,"Train/PolicyRatio/Min":0.9987964630126953,"Train/PolicyStd":0.6735298037528992,"_wandb":{"runtime":7259},"Metrics/EpLen":368.3699951171875,"Time/FPS":158.36666870117188,"Value/cost":2.710101842880249,"Loss/Loss_pi_cost/Delta":91.83304595947266,"Train/PolicyRatio/Max":0.9987964630126953,"Time/Rollout":122.05770111083984,"Time/Total":7258.8154296875,"Train/PolicyRatio/Std":0.02590353786945343,"TotalEnvSteps":1e+06,"Loss/Loss_pi/Delta":0.049889891408383846,"_step":50,"Loss/Loss_cost_critic":2.7280664443969727}