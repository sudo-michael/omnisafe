{"Train/KL":0.0055634030140936375,"_runtime":12295.099941643,"Train/StopIter":40,"Loss/Loss_reward_critic/Delta":2.6655285358428955,"Metrics/EpLen":956.760009765625,"Metrics/EpRet":1501.3370361328125,"Time/FPS":77.1911849975586,"Loss/Loss_pi/Delta":1.2421980500221252e-05,"Metrics/LagrangeMultiplier":0.25720614194869995,"Loss/Loss_pi":-0.004665018990635872,"Train/PolicyRatio/Max":0.9993336796760559,"Time/Total":12294.3798828125,"_timestamp":1.7454858322332478e+09,"Value/reward":153.28810119628906,"Time/Epoch":259.096923828125,"Train/Entropy":0.7764674425125122,"Time/Rollout":122.41265106201172,"Loss/Loss_reward_critic":6.287273406982422,"_wandb":{"runtime":12295},"Train/LR":0,"Time/Update":136.68421936035156,"Metrics/EpCost":15.109999656677246,"TotalEnvSteps":1e+06,"Loss/Loss_cost_critic":0.9689164757728577,"Train/PolicyRatio/Min":0.9993336796760559,"Train/Epoch":49,"Value/cost":1.85200035572052,"_step":50,"Train/PolicyRatio":0.9993337392807007,"Value/Adv":0.19669464230537415,"Train/PolicyRatio/Std":0.012292707338929176,"Loss/Loss_cost_critic/Delta":0.5501013398170471,"Train/PolicyStd":0.5292760729789734}