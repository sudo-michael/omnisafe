{"Train/StopIter":40,"Train/KL":0.005890062544494867,"Loss/Loss_pi":-0.007987989112734795,"Train/PolicyRatio/Max":1.0001885890960693,"Metrics/EpLen":891.3200073242188,"_timestamp":1.7454853664594767e+09,"Value/reward":143.30690002441406,"Loss/Loss_pi/Delta":0.0008740145713090897,"Value/cost":3.801053047180176,"Time/Update":138.88235473632812,"_wandb":{"runtime":12940},"Metrics/EpRet":1345.7545166015625,"_runtime":12940.46443401,"Time/FPS":76.28145599365234,"Train/PolicyRatio":1.0001885890960693,"Time/Epoch":262.1869201660156,"Loss/Loss_cost_critic":2.593094825744629,"Train/PolicyRatio/Min":1.0001885890960693,"Train/Epoch":49,"Time/Rollout":123.30449676513672,"Train/PolicyStd":0.5210328102111816,"Loss/Loss_cost_critic/Delta":-1.5052928924560547,"_step":50,"Metrics/LagrangeMultiplier":0.8701571822166443,"Loss/Loss_reward_critic/Delta":-15.700515270233154,"Loss/Loss_reward_critic":6.627804279327393,"Metrics/EpCost":61.810001373291016,"Train/PolicyRatio/Std":0.012092591263353825,"TotalEnvSteps":1e+06,"Value/Adv":0.17706121504306793,"Train/Entropy":0.762444794178009,"Train/LR":0,"Time/Total":12939.736328125}