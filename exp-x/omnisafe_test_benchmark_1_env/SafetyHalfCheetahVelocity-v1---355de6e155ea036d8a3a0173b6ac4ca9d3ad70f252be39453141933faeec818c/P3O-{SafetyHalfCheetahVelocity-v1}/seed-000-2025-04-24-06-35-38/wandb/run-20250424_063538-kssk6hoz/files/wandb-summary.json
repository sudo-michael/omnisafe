{"Train/PolicyRatio/Max":1.0002986192703247,"Train/StopIter":10,"_runtime":8489.355427773,"Time/Update":44.02674865722656,"Loss/Loss_cost_critic/Delta":0.0067689865827560425,"Train/Entropy":0.4193612337112427,"Train/LR":0,"Train/PolicyRatio":1.0002986192703247,"Loss/Loss_reward_critic":10.495309829711914,"Train/PolicyRatio/Std":0.01002475619316101,"_step":50,"Loss/Loss_pi_cost/Delta":0,"Value/Adv":0.05382847040891647,"_timestamp":1.7455102276666496e+09,"Time/FPS":118.26092529296875,"_wandb":{"runtime":8489},"Time/Epoch":169.11756896972656,"Loss/Loss_reward_critic/Delta":0.7296657562255859,"TotalEnvSteps":1e+06,"Loss/Loss_cost_critic":0.02368246018886566,"Time/Rollout":125.09076690673828,"Loss/Loss_pi/Delta":0.00012987293303012848,"Metrics/EpRet":1034.4356689453125,"Value/cost":0.1762547791004181,"Value/reward":97.62980651855469,"Train/PolicyStd":0.3748650550842285,"Loss/Loss_pi":-0.004314401652663946,"Loss/Loss_pi_cost":0,"Train/Epoch":49,"Time/Total":8488.615234375,"Train/PolicyRatio/Min":1.0002986192703247,"Train/KL":0.004275124054402113,"Metrics/EpLen":1000,"Metrics/EpCost":2.1600000858306885}