{"Train/Epoch":49,"Train/KL":0.007549863774329424,"Value/cost":0.5824503898620605,"Loss/Loss_reward_critic/Delta":3.156984329223633,"Loss/Loss_pi":-0.01641727052628994,"Value/Adv":0.2444176971912384,"Loss/Loss_pi/Delta":0.00573221780359745,"Time/Update":148.45509338378906,"Train/LR":0,"Loss/Loss_cost_critic":0.8476632833480835,"Train/Entropy":0.5766991972923279,"Train/StopIter":40,"Time/Total":10699.689453125,"Train/PolicyRatio/Max":0.9992049932479858,"Loss/Loss_cost_critic/Delta":-0.06279981136322021,"_runtime":10702.176608491,"Metrics/EpCost":1.5800000429153442,"_step":50,"Time/Rollout":155.3211212158203,"Value/reward":-3.8060109615325928,"Loss/Loss_reward_critic":29.534194946289062,"Train/PolicyStd":0.4307980537414551,"Metrics/LagrangeMultiplier":0,"Train/PolicyRatio/Std":0.01296963170170784,"TotalEnvSteps":1e+06,"Metrics/EpLen":87.18000030517578,"Train/PolicyRatio/Min":0.9992049932479858,"_wandb":{"runtime":10702},"_timestamp":1.7454617100016065e+09,"Time/FPS":65.83792877197266,"Metrics/EpRet":18.10879898071289,"Train/PolicyRatio":0.9992049932479858,"Time/Epoch":303.7762756347656}